{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the python package\n",
    "import os\n",
    "# import dynetan\n",
    "from dynetan.toolkit import *\n",
    "from dynetan.viz import *\n",
    "from dynetan.proctraj import *\n",
    "from dynetan.gencor import *\n",
    "from dynetan.contact import *\n",
    "from dynetan.datastorage import *\n",
    "\n",
    "#from numpy.linalg import norm\n",
    "from itertools import islice\n",
    "from scipy import stats\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc13ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnad = DNAdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db46f06",
   "metadata": {},
   "source": [
    "# Load network data obtained by Dynetan module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e740d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataDir = \"Path_to_data_dir\" ## from first notebook output\n",
    "\n",
    "# Path where results will be written (you may want plots and data files in a new location)\n",
    "workDir = \"Path_to_working_dir(data will be saved)\"\n",
    "\n",
    "fileNameRoot = \"from_first_notebook_output\" \n",
    "fullPathRoot = os.path.join(dataDir, fileNameRoot)\n",
    "\n",
    "# Define the segID of the Ligand being studied.\n",
    "ligandSegID = \"SYST\"\n",
    "dnad.loadFromFile(fullPathRoot)\n",
    "dcdVizFile = fullPathRoot + \"_reducedTraj.dcd\"\n",
    "pdbVizFile = fullPathRoot + \"_reducedTraj.pdb\"\n",
    "\n",
    "workUviz = mda.Universe(pdbVizFile, dcdVizFile)\n",
    "# We add this to the object for ease of access.\n",
    "dnad.nodesAtmSel = workUviz.atoms[ dnad.nodesIxArray ]\n",
    "print(dnad.nodesAtmSel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c2639d",
   "metadata": {},
   "source": [
    "A function to calculate pathlengths in terms of edge betweenness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26616b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBCsum(List, Indx):\n",
    "        bc = 0\n",
    "        # Iterate over edges in the path\n",
    "        for i in range(len(List)-1):\n",
    "            node1 = List[i]\n",
    "            node2 = List[i+1]\n",
    "            if node1 > node2:\n",
    "                btw = (dnad.btws[Indx][( node2, node1)])/maximumBetweeness\n",
    "            else:\n",
    "                btw = (dnad.btws[Indx][( node1, node2)])/maximumBetweeness\n",
    "            bc += btw\n",
    "        return bc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde3fd7d",
   "metadata": {},
   "source": [
    "## write down the opt- and sub-opt paths between source and sink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d4e49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source = list(range(1187,1191)) ## ## resid list to define source nodes\n",
    "sink = [1047, 1052, 471, 476] ## resid list to define sink nodes\n",
    "\n",
    "# Initialize variable with high value.\n",
    "minimumBetweeness = 100\n",
    "# Initialize variable with low value.\n",
    "maximumBetweeness = -1\n",
    "\n",
    "for pair,btw in dnad.btws[0].items():\n",
    "    if btw < minimumBetweeness:\n",
    "            minimumBetweeness = btw\n",
    "    if btw > maximumBetweeness:\n",
    "            maximumBetweeness = btw\n",
    "\n",
    "# Normalize the value.\n",
    "minimumBetweeness /= maximumBetweeness\n",
    "# Determine how many extra sub-optimal paths will be written.\n",
    "numSuboptimalPaths = 5   \n",
    "\n",
    "\n",
    "pathListFileD = open(os.path.join(workDir, \"Name_the_output_containing_all_paths.dat\"), \"w\")\n",
    "\n",
    "for srcNode in source:\n",
    "    print(srcNode)\n",
    "    for trgNode in sink:\n",
    "\n",
    "        tmpList = getSelFromNode(srcNode,dnad.nodesAtmSel, atom=True).split()\n",
    "        srcNodeSel = \"\".join([tmpList[1],tmpList[4],tmpList[10]])\n",
    "\n",
    "        tmpList = getSelFromNode(trgNode,dnad.nodesAtmSel, atom=True).split()\n",
    "        trgNodeSel = \"\".join([tmpList[1],tmpList[4],tmpList[10]])\n",
    "\n",
    "        for winIndx in range(dnad.numWinds):\n",
    "            \n",
    "            normCorMat = copy.deepcopy( dnad.corrMatAll[winIndx,:,:] )\n",
    "            normCorMat /= normCorMat.max()\n",
    "            \n",
    "            allPaths = []\n",
    "            \n",
    "            # Reconstructs the optimal path from Floyd-Warshall algorithm\n",
    "            pathFW = nx.reconstruct_path(srcNode, trgNode, dnad.preds[winIndx])\n",
    "            allPaths.append(pathFW)\n",
    "\n",
    "#             Behind the scenes, use Dijkstra algorithm to find sub-optimal paths\n",
    "            for pathSO in islice(nx.shortest_simple_paths(dnad.nxGraphs[0], \n",
    "                                                srcNode, trgNode, weight=\"dist\"), 1, numSuboptimalPaths + 1):\n",
    "                allPaths.append(pathSO)\n",
    "    \n",
    "                    \n",
    "            ### ---- Write all the paths calculated so far ---- ####      \n",
    "            \n",
    "         # Create a counter of number of paths that go though each edge, among all (sub-)optimal path(s).\n",
    "            pathCounter = defaultdict(int)\n",
    "            for pathIndx, pathIter in enumerate(allPaths):\n",
    "                # Iterate over edges in the path\n",
    "                for i in range(len(pathIter)-1):\n",
    "\n",
    "                    node1 = pathIter[i]\n",
    "                    node2 = pathIter[i+1]\n",
    "\n",
    "                    pathCounter[(node1, node2)] += 1\n",
    "#                 print(pathCounter)\n",
    "\n",
    "            # Normalize the count\n",
    "            maxCount = np.max(list(pathCounter.values()))\n",
    "            for pair, count in pathCounter.items():\n",
    "                pathCounter[pair] = count/maxCount\n",
    "\n",
    "            for pathIndx, pathIter in enumerate(allPaths):\n",
    "            # Iterate over edges in the path\n",
    "                for i in range(len(pathIter)-1):\n",
    "\n",
    "                    node1 = pathIter[i] + 1\n",
    "                    node2 = pathIter[i+1] + 1\n",
    "\n",
    "                     # Get the betweeness value\n",
    "                    try:\n",
    "                        if node1 > node2:\n",
    "                            btw = (dnad.btws[winIndx][( node2, node1)])\n",
    "                        else:\n",
    "                            btw = (dnad.btws[winIndx][( node1, node2)])\n",
    "                    except:\n",
    "                        # If one could not be calculated (very few paths going though this edge)\n",
    "                         # set an arbitrarily low value.\n",
    "                        btw = minimumBetweeness\n",
    "\n",
    "\n",
    "                    string = \"{} {} {} {} {} {}\".format(node1, node2, \n",
    "                                                         normCorMat[ node1, node2], \n",
    "                                                         btw/maximumBetweeness, pathCounter[(node1, node2)], \n",
    "                                                         pathIndx)\n",
    "\n",
    "                    pathListFileD.write( string + \"\\n\" )\n",
    "\n",
    "pathListFileD.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef391664",
   "metadata": {},
   "source": [
    "## Calculate occurance of nodes across optimal and Sub-optimal pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5497e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s1 = np.loadtxt(os.path.join(workDir,'file_containing_all_paths.dat'))\n",
    "print(s1.shape)\n",
    "print(s1.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863766c",
   "metadata": {},
   "source": [
    "Occurence of residues in top-ranked paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    "     \n",
    "    # insert the list to the set\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    return unique_list\n",
    "\n",
    "source_sink = [1048, 1053, 477, 472, 1187, 1188, 1189, 1190] # Provide the whole list of resids previously defined as sources and sinks\n",
    "path_ranks = [0, 1, 2, 3] # provide ranks of paths across which you want to calculate the occurence of nodes\n",
    "\n",
    "def get_occurence(data,output_filename, cutoff=0): ##data = the loaded path data as np matrix\n",
    "    \n",
    "    with open(os.path.join(workDir,output_filename), 'w') as h:\n",
    "        resid1 = []\n",
    "        for i in list(range(data.shape[0])):\n",
    "            if data[i,5] in path_ranks:\n",
    "                resid1.append(int(data[i,1]))\n",
    "        print(\"shape of residue list:\", len(resid1))\n",
    "        print(\"No of unique residues in the list:\", len(unique(resid1)))\n",
    "        print(\"Unique residues are:\", unique(resid1))\n",
    "        for x in unique(resid1):\n",
    "            if x not in source_sink: ## Excluding the source and sink resids\n",
    "                if int(resid1.count(x)) > cutoff: ## filter only those residues with ocuurance > cutoff\n",
    "                print(x, int(resid1.count(x)), file=h)\n",
    "                \n",
    "\n",
    "get_occurence(s1, \"nodefrequency.dat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb990b96",
   "metadata": {},
   "source": [
    "Sort \"nodefrequency.dat\" by resid ascending order and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fdfd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sort -n -k1 nodefrequency.dat > nodefrequency_sorted.dat\n",
    "s2 = np.loadtxt(os.path.join(workDir,\"nodefrequency_sorted.dat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = []\n",
    "for i in list(range(s2.shape[0])):\n",
    "        resid.append(str(s2[i,0]))\n",
    "count = []\n",
    "for i in list(range(s2.shape[0])):\n",
    "        count.append((s2[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80f0f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # high resolution\n",
    "## Plotting\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.bar(resid, count, color ='mediumpurple',width = 0.8, label='Name_your_label')\n",
    "\n",
    "# plt.axis([1153, 1200, -0.4, 0.4])\n",
    "\n",
    "plt.xlabel(\"Residues\", fontname = 'Arial', fontsize = 14, fontweight = 'bold')\n",
    "plt.ylabel(\"Occurance in Optimal Paths\", fontname = 'Arial', fontsize = 14, fontweight = 'bold')\n",
    "plt.yticks(fontname = 'Arial', fontsize ='large')\n",
    "plt.xticks(\n",
    "    rotation=90, \n",
    "    horizontalalignment='center',\n",
    "    fontweight='light',\n",
    "    fontsize='large'  \n",
    ")\n",
    "plt.legend(prop={\"size\":14}, frameon=False)\n",
    "plt.savefig(os.path.join(workDir,\"nodefrequency_sorted.png\"), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dbece0",
   "metadata": {},
   "source": [
    "## Calculate path-length across all optimal and sub-optimal pathways between pair of source-sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d11b347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "s = np.loadtxt(os.path.join(workDir,\"file_containing_all_paths.dat\"))\n",
    "\n",
    "subpathID = [0, 1, 2, 3, 4, 5] ## Give ID of path ranks ; if want to get lengths of only the optimal path, subpathID = [0]\n",
    "\n",
    "def subpathlength(data, path_id_list):\n",
    "    d = 0\n",
    "    c = 0\n",
    "    pathlengths = []\n",
    "    pathcount = []\n",
    "    \n",
    "    for j in subpathID:\n",
    "        for i in list(range(s.shape[0])):\n",
    "            if (s[i,5] == j):\n",
    "                d += s[i,3]\n",
    "                c += 1\n",
    "            elif d != 0:\n",
    "                    pathlengths.append(d)\n",
    "                    pathcount.append(c)\n",
    "                    d = 0\n",
    "                    c = 0\n",
    "            else:\n",
    "                d = 0\n",
    "                c = 0\n",
    "    print(\"shape of pathlengths list:\", len(pathlengths))\n",
    "    print(\"Path lengths:\", pathcount)\n",
    "    print(\"Average path length (betweenness estimate, pathlength estimate):\", (sum(pathlengths) / len(pathlengths)), (sum(pathcount) / len(pathcount)))\n",
    "    return pathcount\n",
    "\n",
    "pathcount_sys = subpathlength(s,subpathlength)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64260f2c",
   "metadata": {},
   "source": [
    "Plotting distribution of pathcount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370cf89e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # high resolution\n",
    "## Plotting\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "import seaborn as sb\n",
    "ax = sb.kdeplot(pathcount_sys, color=\"springgreen\", shade=True, alpha=0.5, linewidth=1, label = 'Name_your_label')\n",
    "\n",
    "#Setting the border of the box\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "# plt.xlim(-2, 6)\n",
    "# plt.ylim(0, 0.35)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Opt Path length\", fontname = 'Arial', fontsize = 14, fontweight = 'bold')\n",
    "plt.ylabel(\"Density\", fontname = 'Arial', fontsize = 14, fontweight = 'bold')\n",
    "plt.xticks(fontname = 'Arial', fontsize = 12)\n",
    "plt.yticks(fontname = 'Arial', fontsize = 12)\n",
    "plt.legend(prop={\"size\":14}, loc = \"upper right\",frameon=False)\n",
    "# plt.legend(prop={\"size\":14}, frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(workDir,\"distribution_of_pathcount.png\"), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b364ca",
   "metadata": {},
   "source": [
    "## Calculate inter-domain betweennesses which is a more robust data than intercommunity betweenness data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5674d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to assign nodes to a specific domain\n",
    "def search(lst, value):\n",
    "    for i in range(len(lst)):\n",
    "        if value in lst[i]:\n",
    "            return i\n",
    "\n",
    "# define a list of domain residue ranges\n",
    "domain = [list(range(167,307)), list(range(494, 710)), list(range(710, 762))+list(range(906, 1096)), list(range(762, 906)), list(range(1451, 1470)), list(range(1362, 1381))] \n",
    "\n",
    "\n",
    "def interdomain(domain_list, output_prefix):\n",
    "    \n",
    "    # create a 2d np.array to fill domain betweenness\n",
    "    domain_bet = np.zeros(shape=(len(domain), len(domain)))\n",
    "    \n",
    "    for winIndx in range(dnad.numWinds):\n",
    "\n",
    "        normCorMat = copy.deepcopy( dnad.corrMatAll[winIndx,:,:] )\n",
    "        normCorMat /= normCorMat.max()\n",
    "\n",
    "        for pair in np.asarray( np.where( np.triu(normCorMat[:,:]) ) ).T:\n",
    "            \n",
    "           ## A running matrix to fill out domain betweenness\n",
    "            running_dom = np.zeros(shape=(len(domain), len(domain)))\n",
    "            node1 = pair[0]\n",
    "            node2 = pair[1]\n",
    "\n",
    "          # if the nodes doesn't belong to the same domain, calculate the betweenness between them\n",
    "            max_list = max[item for sublist in domain_list for item in sublist]\n",
    "            if (node1 < max_list) and (node2 < max_list):\n",
    "                if search(domain,node1) != search(domain,node2):\n",
    "                    running_dom[search(domain,node1), search(domain,node2)] = normCorMat[ node1, node2]\n",
    "                    running_dom[search(domain,node2), search(domain,node1)] = normCorMat[ node1, node2]\n",
    "                    domain_bet = np.add(domain_bet, running_dom) # updating after each frame\n",
    "\n",
    "    print(domain_bet)\n",
    "    domain_bet_round = np.round_(domain_bet, decimals = 3)\n",
    "    np.save(os.path.join(workDir, output_prefix+\".npy\"), domain_bet_round)\n",
    "    np.savetxt(os.path.join(workDir, output_prefix+\".csv\"), domain_bet_round, fmt='%.3f', delimiter=\",\")\n",
    "\n",
    "\n",
    "    \n",
    "# Example: interdomain(domain,\"interdomain_betweenness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede86db",
   "metadata": {},
   "source": [
    "# This section reads the interdomain betweenness matrices and calculate the difference in interdomain betweennesses of 2 comparable systems..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87adad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## differential domain betweenness #######\n",
    "def diff_domain_between(filename1, filename2, outputfilename): #all csv format files e.g.: filename1= interdomain_betweenness_s1.csv\n",
    "    \n",
    "    import numpy as np\n",
    "    from numpy import genfromtxt\n",
    "    s1 = genfromtxt(os.path.join(workDir, filename1), delimiter=',') # SYS-1\n",
    "    s2 = genfromtxt(os.path.join(workDir, filename2), delimiter=',') # SYS-2\n",
    "    \n",
    "    t1 =  np.zeros(shape=(s1.shape[0], s1.shape[1]))\n",
    "\n",
    "    for i in range(s1.shape[0]):\n",
    "        for j in range(s1.shape[1]):\n",
    "            t1[i,j] = s1[i,j] - s2[i,j]\n",
    "    print(\"system-1:\", s1)\n",
    "    print(\"system-2:\", s2)\n",
    "    print(\"Difference:\", t1)\n",
    "    np.savetxt(os.path.join(workDir, outputfilename), t1, fmt='%.3f', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6e631",
   "metadata": {},
   "source": [
    "## For read graph and visulaize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae0f7b",
   "metadata": {},
   "source": [
    "Load the interdomain betweenness data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c23c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "interdomain_bet = np.genfromtxt(os.path.join(workDir, \"differential_betweenness_filename.csv\"), delimiter=\",\")\n",
    "\n",
    "print(interdomain_bet)\n",
    "print(interdomain_bet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb2b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.triu(interdomain_bet)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ef143",
   "metadata": {},
   "source": [
    " Specify domain names for which intedomain betweenness is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcaa253",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_names = ['NTD', 'Helical-1', 'HEPN1-I', 'Helical-2', 'HEPN1-II', 'Linker', 'HEPN2', 'crRNA'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22cc49",
   "metadata": {},
   "source": [
    "Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653164c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# fig, ax = plt.subplots()\n",
    "# ig.plot(g, target=ax)\n",
    "import igraph as ig \n",
    "from igraph import *\n",
    "\n",
    "# get the row, col indices of the non-zero elements in your adjacency matrix\n",
    "conn_indices = np.where(b)\n",
    "\n",
    "# get the weights corresponding to these indices\n",
    "weights = b[conn_indices]\n",
    "\n",
    "# a sequence of (i, j) tuples, each corresponding to an edge from i -> j\n",
    "edges = zip(*conn_indices)\n",
    "\n",
    "# initialize the graph from the edge sequence\n",
    "G = ig.Graph(edges=edges, directed=False)\n",
    "\n",
    "color_list = []\n",
    "for i in weights:\n",
    "    if i < 0:\n",
    "        color_list.append('red')\n",
    "    else:\n",
    "        color_list.append('cyan')\n",
    "# assign node names and weights to be attributes of the vertices and edges\n",
    "# respectively\n",
    "G.vs['label'] = node_names\n",
    "G.es['color'] = color_list\n",
    "G.es['weight'] = np.absolute(weights)\n",
    "\n",
    "# G.es['color'] = color_list\n",
    "\n",
    "# I will also assign the weights to the 'width' attribute of the edges. this\n",
    "# means that igraph.plot will set the line thicknesses according to the edge\n",
    "# weights\n",
    "G.es['width'] = np.absolute(weights)\n",
    "\n",
    "\n",
    "\n",
    "# plot the graph, just for fun\n",
    "# igraph.plot(G, layout=\"rt\", labels=True, margin=80, target=ax)\n",
    "layout = G.layout(\"circle\")\n",
    "visual_style = {}\n",
    "visual_style[\"vertex_size\"] = 70\n",
    "visual_style[\"vertex_color\"] = \"wheat\"\n",
    "visual_style[\"labels\"] = True\n",
    "visual_style[\"margin\"] = 80\n",
    "visual_style[\"layout\"] = layout\n",
    "\n",
    "ig.plot(G,\"os.path.join(workDir, \"differential_betweenness_filename.png\"), **visual_style)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
